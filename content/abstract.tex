Stream Reasoning research field is grown enough to prove that reasoning upon rapidly changing information is possible. Many working solutions were proposed: The number of RSP Engines, system capable to handle at semantic level information flows encoded in RDF, is increasing. Now the Stream Reasoning community is working on the standardisation of the methods and tools that supported the development of those solutions. Moreover, it is mandatory to provide an evaluation of RSP Engines, which allows to understand how these systems perform in real uses cases. Recent works in the filed has pursued this goal, providing many Benchmarking frameworks for evaluating RSP Engines. Further analysis of RSP Engines have point out the challenges involved by the Stream Reasoning research. They have posed the basis for a proper evaluation of such a system, describing in detail where these works have failed and where the can be improved. 

In parallel, the entire Computer Science (CS) community has questioned itself about the nature of its own research. The answers have shown the affinity of CS research to an Engineering epistemology. They have also evinced the concrete lacuna of the traditional engineering approach, which focus on system evaluation and not only on their design and development. The lacks of an empirical approach can be ascribed to the complex nature of the software systems. However, other research areas have found methods to face complex studies that can not be easily modelled.

The Stream Reasoning research suffers from the same lack. The limitations of the existing benchmarking proposal have proved the need of empirical evaluation of RSP Engine systems, upon which can be easily built comparisons. We borrow from the aerospace engineering the idea of an engine test stand. It is an automatic facility, which allows to design experiments and to execute them on any RSP Engine, under controlled conditions. Experiment properties like reproducibility, repeatability and comparability represent the pillars upon which we can start the empirical evaluation of RSP Engines. Thus, we formulate the following research question: "\textit{Can an engine test stand, together with queries, datasets and methods, support SCRA for Stream Reasoning?}"

IN this thesis we answer this question. We propose \namens, an open source framework that enable the  Systematic Comparative Approach to the research of RSP Engine. It consists first in and engine Test Stand, the analogous of the aerospace engineering facility in the Stream Reasoning context. \name contains also four Baselines, which are naive implementations of RSP Engines. They represent  simple terms of comparison that can start the comparative research. Finally, \name enable the comparative analysis of RSP Engines at different levels trough the Analyser: a set of methods and tools, organised into an investigation stack.
