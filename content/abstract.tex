Stream Reasoning research field is grown enough to prove that reasoning upon rapidly changing information is possible. The number of working proposals for RSP Engines, systems capable to handle at semantic level RDF-encoded information flows, is increasing. Now the Stream Reasoning community is working on the standardisation of the methods and tools that supported the development of those solutions. Moreover, it is mandatory to provide an evaluation of RSP Engines, which allows to understand how these systems perform in real uses cases. 
Recent works in the filed \cite{Zhang2012, LePhuoc2012c, DBLP:conf/semweb/DellAglioCBCV13} has pursued this goal, providing many Benchmarking frameworks for evaluating RSP Engines. Further analysis of RSP Engines have point out the challenges involved by the Stream Reasoning research. They have posed the basis for a proper evaluation of such a system, describing in detail where these works have failed and where the can be improved \cite{DBLP:conf/esws/ScharrenbachUMVB13}. 

In parallel, many  Computer Science (CS) research fields have tried to understand the nature their own research. The related studies \cite{Tichy:1995:EEC:209090.209093, Wainer:2009:EEC:1518331.1518552} have shown the affinity of many CS research fields to an Engineering epistemology. But they have also evinced and criticised the concrete differences with other engineering research areas, which focus on evaluation and not only on the design and development of the proposed systems. The lacks of an empirical approach can be ascribed to the complex nature of the software systems. However, it is possible to face complex case studies which are not be easily modelled. Social science and economy researchers have found methods to deal with them.

The Stream Reasoning research suffers from the same lack. The limitations of the existing benchmarking proposals have proved that the empirical evaluation of RSP Engine systems is just at the beginning, and what is missing in an infrastructure which allows to compare the evaluations of RSP Engines. We borrow from the aerospace engineering the idea of an engine test stand. It is an automatic facility to design experiments and to execute them on any engine, under controlled conditions. Experiment properties like reproducibility, repeatability and comparability represent the pillars upon which we can start the empirical evaluation of RSP Engines. Thus, we formulate the following research question: "\textit{Can an engine test stand, together with queries, datasets and methods, support SCRA for Stream Reasoning?}"

In this thesis we propose \namens, an open source framework that enable the  Systematic Comparative Approach to the research of RSP Engine. It consists first in and engine Test Stand, the analogous of the aerospace engineering facility in the Stream Reasoning context. \name contains also four Baselines, which are naive implementations of RSP Engines. They represent  simple terms of comparison that can start the comparative research. Finally, \name enable the comparative analysis of RSP Engines at different levels trough the Analyser: a set of methods and tools, organised into an investigation stack.
