Stream Reasoning research field is grown enough to prove that reasoning upon rapidly changing information is possible. The number of implemented RDF Stream Processing (RSP) Engine, systems capable to handle at semantic level RDF-encoded information flows, is increasing. Now the Stream Reasoning community is working on the standardisation of the methods and tools that supported the development of those solutions. Moreover, it is mandatory to provide an evaluation of RSP Engines, which allows to understand how these systems perform in real uses cases. 
Recent works in the filed \cite{Zhang2012, LePhuoc2012c, DBLP:conf/semweb/DellAglioCBCV13} pursued this goal, providing many benchmarks for evaluating RSP Engines. Further analysis of RSP Engines \cite{DBLP:conf/esws/ScharrenbachUMVB13} pointed out the challenges involved by the Stream Reasoning research. \cite{DBLP:conf/esws/ScharrenbachUMVB13} posed the basis for a proper evaluation of such a system, describing in detail where these works have failed and where the can be improved.

In parallel, many  Computer Science (CS) research fields tried to understand the nature of their own research. The related studies \cite{Tichy:1995:EEC:209090.209093, Wainer:2009:EEC:1518331.1518552} shown the affinity of many CS research fields to an Engineering epistemology. But they also evinced and criticised the concrete differences with other engineering research areas, which focus on evaluation of the proposed systems and not only on their design and development. The lacks of an empirical approach can be ascribed to the complex nature of the software systems. However, it is possible to face complex case studies which are not be easily modelled. Social science and economy researchers found methods to deal with them.

The Stream Reasoning research suffers from the same lack. The limitations of the existing benchmarking proposals proved that the empirical evaluation of RSP Engines is just at the beginning. What is still missing in an infrastructure that allows to compare, maybe automatically, the performances of many RSP Engines. We borrow from the aerospace engineering the idea of an engine test stand, which is an automatic facility for engine testing. A test stand allows to design experiments and to execute them, evaluating engines in a controlled environment. %Experiment properties like reproducibility, repeatability and comparability represent the pillars upon which we can start the empirical evaluation of RSP Engines.
Thus, we formulate the following research question: "\textit{Can an engine test stand, together with queries, datasets and methods, support Systematic Comparative Research Approach for Stream Reasoning?}"

In this thesis we propose \namens, an open source framework that enables the Systematic Comparative Approach in the Stream Reasoning research field. \name consists in:  an RSP Engine Test Stand, which emulates the aerospace engineering facility in the Stream Reasoning context; the Analyser, which enables the Systematic Comparative Approach trough a set of methods and tools for the investigation, hierarchically organised into an stack; and, finally, four naive implementations of RSP Engines, called Baselines, which represent simple terms of comparison upon which  start the comparative research.