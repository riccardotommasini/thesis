In this thesis work we have presented \name -- an open source framework for empirical evaluation of RSP Engines. \name aims enabling Systematic Comparative Approach in the Stream Reasoning research field trough an RSP Engine \textsc{Test Stand}, four Baselines of RSP Engines, and an \textsc{Analyser}. 

The motivations that led this work are included in Chapter \ref{chap:problem-settings}, while in Chapter \ref{chap:heaven} we described \name design and in Chapter \ref{chap:implementation-experience} we detailed the implementation of the \textsc{Test Stand}, the four Baselines and the investigation methods which compose the \textsc{Analyser}. Finally, we provided an empirical proof of \name potential in Chapter \ref{chap:evaluation}. Within the evaluation we compared the results of two experimental sets, SOAK and Stress tests, executed on the Baselines implementations. We learnt that, even when RSP Engines are extremely simple (e.g., the baselines), it is hard to demonstrate hypothesis formulated only from a theoretical knowledge. Thus empirical evaluation is required, because results emphasised the importance of conducting comparative research based on controlled experimental conditions and, thus, the need for a open source\footnote{\url{https://github.com/streamreasoning/HeavenTeststand}} framework like \namens.

The focus on the experimental infrastructure is the main difference between \name and previous work. While SRbench and LSbench focus on RDF streams and a suite of continuous SPARQL query, \name allows to compare RSP Engines based on any RDF stream, ontology, continuous query and entailment regime. It even enables to run experiments connecting to live data streams as those used in \cite{DBLP:conf/semweb/BalduiniVDTPC13}.	

In this chapter we recap this thesis works, presenting in Section \ref{sec:research-question-conclusion} our Research Question; in Section \ref{sec:research-results-conclusion} we briefly detail \namens, our work, which issues it involved and how we answer the research question. Finally, in Section \ref{sec:research-fw-conclusion} we point out \name limitations and future works of this thesis.

\section{Systematic Comparative Research Approach of RSP Engines}\label{sec:research-question-conclusion}

Stream Processing research field is growing and the number of techniques to semantically handle data stream is increasing. RDF Stream Processing Engines, a.k.a. RSP Engines, are systems able to answer continuous extensions of SPARQL queries over RDF Streams. Due to their complexity, a systematic comparison of such systems, under repeatable conditions, is hard. 

It is worth to note that, despite the Engineering epistemology of the Computer Science works, it is still present the lack of a Systematic Comparative Research Approach (SCRA) \cite{Tichy:1995:EEC:209090.209093}. SCRA is typical of those research areas which have to face very complex systems, and have difficulty to simplify the models. Architectural analysis are useful, but they are not sufficient to evaluate RSP Engine, because their behaviour must be studied during the execution. For this reason, the Stream Reasoning community has tried to define and develop solution to evaluates RSP Engines \cite{DBLP:conf/esws/ScharrenbachUMVB13}. Recent works like \cite{Zhang2012, LePhuoc2012c, DBLP:conf/semweb/DellAglioCBCV13} supported this approach with queries, dataset and methods. However the SR community still lacks an experimental infrastructure which enables the comparison of RSP Engines independently from RDF Stream, ontology, continuous query and entailment regime.  From aerospace engineering we borrow the idea of engine test stand: a facility to develop engine trough systematic testing under precise experimental conditions. Thus, we can formulate our research question as follow:\\

\textit{”Can an engine test stand, together with queries, datasets and methods, support Systematic Comparative Research Approach for Stream Reasoning?”}

%esperiment, reproducibility, repetability and comparability to enable the SRCA by a test stand
%simple terms of comparison to support the comparative research: SERE properties
%test stand design and implmentation fulfill the requirements posed
%Analyser consist in the definition of an analysis stack which extend% the traditonal top-down investiagtion based on hypothesis formulated on the model, trough empirical findings ad different level of analysis.
%We prove the value of the empirical resarch heaven test stand enables by evaluating the baselines
%we see that even for simple system like the baselines, which model is known, upredictable results may rise
%now it is possible to improve existing RSP engine model trough finding provide by the different analysi levels of the stack

In this thesis we answered such research question presenting \name, an open source framework for SCRA of RSP Engines. In the following we provides evidence of the positive results of our work, describing each phases it involves as how they are organised in this thesis.  

In Chapter \ref{chap:problem-settings} we describe how to borrow this idea of a test stand in the Stream Reasoning research field, with the goal of RSP Engine evaluation. We exploit the traditional experiment definition to grant the rigorous and systematic test of an RSP Engine. Three main experiment properties, \textit{reproducibility}, \textit{repeatability} and \textit{comparability}, allow us to formulate the requirements that a Test Stand for RSP Engine must fulfil, in order to answer our research question. SCRA, due to its case-oriented nature, demands simple terms of comparison, namely baselines, to exploit for initial evaluation examples. In Chapter \ref{chap:problem-settings} we detail which properties a baselines must have and we formulate them as  requirements for our work.

In Chapter \ref{chap:heaven} we describe \name design. We explain how the \textsc{Test Stand} and the Baselines should be to fulfil the requirements we posed. We introduce also the idea of the \textsc{Analyser} as an investigation stack, which extends the research of RSP Engine from the traditional hypothesis based approach to the empirical and comparative one. The higher levels of the investigation stack provide a statistical evaluation of experiment results, while the lower levels focus on RSP Engine dynamics offering an overview of the engine behaviour over all the experiment execution (further details of \name implementation and \textsc{Analyser} investigation stack can be found in Chapter \ref{chap:implementation-experience}).

%Our research question asks to demonstrate if a \textsc{Test Stand} for RSP Engine can support the SCRA for Stream Reasoning. 
In Chapter \ref{chap:evaluation} we show how the traditional top-down analysis are not enough for evaluating complex systems like RSP Engines, even in case of naive implementations. Our evaluation exploits an experimental set composed by SOAK Tests and Step Response Stress Tests, executed on the Baselines implementations that we included in \name framework. The results of the analysis show how the traditional research, which formulate hypothesis only on the RSP Engine model knowledge, is still meaningful, but it can be improved trough an infrastructure like \name \textsc{Test Stand}. The evaluation conducted in Chapter \ref{chap:evaluation} has shown that it is hard to demonstrate even naive hypothesis. RSP Engine dynamics can be only partially investigated from the statistical viewpoint We need further knowledge about the RSP Engine dynamics, which means observing their behaviour at once and over the entire execution of an experiment. In this way it is possible to apply SCRA at any details level it requires.\\

\noindent \name allows to drill down the analysis over an investigation stack which covers all the aspects of the dynamic system performance analysis. Previous works have defined how to evaluate RSP Engine systems \cite{DBLP:conf/esws/ScharrenbachUMVB13} and many attempts has been proposed \cite{Zhang2012, LePhuoc2012c, DBLP:conf/semweb/DellAglioCBCV13}. 

Thus, we can positively answer   our research question, stating that \name sustains SCRA and extends the traditional top-down analysis. The proposed queries and dataset can be evaluated systematically trough \namens, It is now possible to improve existing theoretical models trough the empirical findings \name points out, which were not easily available before.


\section{Limitations And Future Works}\label{sec:research-fw-conclusion}

During \name development we faced many issues related to the heterogeneous nature of RSP application domains. These concerns limit our work in different ways. They influence \name development in term of both design and implementation. Moreover, our research of RSP Engines  is actually restricted to \name Baselines within an extremely controlled experimental setting.

The limitations on \name design and its implementation must be faced improving its models and further developing the current implementation of the \textsc{Streamer}, \textsc{ResultCollector} and the \textsc{Test Stand External Structure}. On the other hand, the restrictions on the research of RSP Engine require to exploit \name \textsc{Test Stand} in order to pursue the analysis. Finally, we consider a further possible contribution the continuation of the research of the Baselines, which has its own scientific value, as Chapter \ref{chap:evaluation} partially evidenced.

Due to these limitations, the future works and possible extensions of \name belong to the following categories:
\begin{itemize}
\item \textit{Research of RSP Engine} - it involves the empirical evaluation of RSP Engine and the comparison of benchmarking results, which are our main research interests. Thus, we plan to support our research trough \namens.
\item \textit{Software Engineering and Development} - it involves future works focuses on the different aspects of \name software, which is extendible by design.
\item \textit{Research of Baselines} - it aims to provide a complete evaluation of the Baselines as simple terms of comparison for mature RSP Engines.
\end{itemize}

\noindent We aim extending the \textit{Research Work} creating a ready-to-use benchmarking suite built upon \namens, which allows to test any RSP Engine with a set of well defined experiments. Form preliminary studies we know that to cover the most important uses case an essential set of experiment must include the following tests:
\begin{itemize}
\item[T1] SOAK
\item[T2] Stress Step
\item[T3] Stress Sine Wave
\item[T4] Random Distribution (e.g., gaussian and exponential)
\end{itemize}

The experiments definition still follows the tuple $<\mathcal{E},\mathcal{D},\mathcal{T},\mathcal{Q}>$. The ready-to-use benchmarking suite will give the user the possibility to execute those test on his own RSP Engine. Moreover it should include \name Baselines as example of $\mathcal{E}$ and simple terms of comparison for benchmarking results.

SOAK Test [T1] and Stress Step Test [T2] are already part of this thesis work in a restricted form, while the other ones are not implemented yet. 

We develop [T1] and [T2] registering to our $\mathcal{E}$ as queries $\mathcal{Q}$ variations of the identity query, which a differ for window size $\omega$, because in the current stage of development it is possible to configure only the ontology and the entailment regime of the Baselines. We intend to continue the development of the Baselines, adding the possibility to register one or more continuous queries into them and exploiting more complex entailment regime than $\rho$DF. 

We have also to define which queries $\mathcal{Q}$ to include in all the experiments, considering many works in the field \cite{DBLP:conf/esws/ScharrenbachUMVB13, Zhang2012, LePhuoc2012c, DBLP:conf/semweb/DellAglioCBCV13}.

The current experiment sets [T1] and [T2] exploit LUBM ontology as $\mathcal{T}$ and the RDF Stream $\mathcal{D}$ is generated trough a module, the \textsc{RDF2RDFStream}, which adapts LUBM data to a streaming scenario (See Chapter \ref{chap:implementation-experience}). In order to generate data for all the remaining test sets [T3] and [T4] we have to extend the \textsc{RDF2RDFStream} to generate: a random flow with a given distribution (e.g., gaussian and exponential), and a sine wave flow (to mimic the periodic changes in the flow rates observed on social media streams \cite{DBLP:conf/semweb/BalduiniVDTPC13}).

Independently from the experiment set, we indent to extend the metrics gathered by \name \textsc{Test Stand}. Quoting from \cite{DBLP:conf/esws/ScharrenbachUMVB13} the possible extensions are:
\begin{itemize}
\item Response time over all queries (Average/$1^th$ Percentile/Maximum).
\item Maximum input throughput in terms of number of data element in the input stream consumed by the system per time unit.
\item Minimum time to accuracy and the minimum time to completion for all queries.
\end{itemize}

As we stated in Chapter \ref{chap:evaluation}, the content of the current window influences the RSP Engine performances for two factors: the window size before the reasoning and after. The second metrics is relevant for the RSP Engine evaluation. When the system has to handle a big number of outgoing triples we can observe a degradation in terms of memory and latency. Thus, an evaluation of the number of inferred triples w.r.t the window content at time $t$ allows to weight the engine performance results in relation to the input RDF Stream, helping to eliminate outliers and properly evaluate RSP Engines. Moreover, this observation opens new scenarios in the Stress Testing design, where the stress factor depends on the reasoning potential of the current window w.r.t a certain entailment regime.\\

\noindent Future works on \textit{Software Engineering and Development} regard the \textsc{Analyser}.  We aim to complete automate the analysis procedure, involving at least the current measurement set and tools. 

As a long term goal, we intend to standardise and publish the entire tool-set which supports analysis methods presented in Chapter \ref{chap:heaven}. We are imagining the \name \textsc{Analyser} as a Web-based environment where all existing RSP engines are already available and those: who want to run experiments, have just to configure them picking up one or more RDF streams, ontologies, and queries; who want to compare the results of its own RSP Engine can upload the raw data and wait for the evaluations results. A visual facility to compare different experiments and the publication of result experiments as linked data would complete this environment. \\


\noindent Finally, by evaluating the Baselines to prove \name potential we identified an intrinsic scientific value of this analysis. We would like to continue the \textit{Research of Baselines}, proving another contribution beyond the developments proposed above. In order to study the problem of responsiveness we have to add four alternative implementation of the Baselines, which do not exploit the external time control. The Baselines should be evaluated by [T1] T2] [T3] and [T4] but also with real data and a $\mathcal{T}$ different from LUBM, for example exploiting LS Bench queries and data to design an experiment set. 
