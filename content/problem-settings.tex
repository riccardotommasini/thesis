\chapter{Problem Settings}

In this chapter we present firstly the question that leads our research and the motivations which sustain it. Next we extend in Section x  the requirements posed by the research question and the issues it involves. In the end we provide the technical formalization of those requirements and why our work must cover them, which issue they solves and which limitation we have to face to.

\section{Overview}
Stream Reasoning research filed wonders to achieve a tight integration of known reasoning systems and DSMS. An RDF Stream Processing Engine, shortly RSP Engine, is the abstraction that realizes this integration and can exploit reasoning techniques upon rapid changing information \cite{Background SW, DSMS, SR}. Consequently it is able to  manage rapidly changing worlds at the semantic level and answer typical Semantic Web complex queries. The Stream Reasoning community is working on the standardization of a protocol to talk with RSP Engines, which actually can be implemented applying a number of RSP techniques. Their number is increasing together with the need of shared practices and tools to perform analyses and evaluations. As stated in Chapter 2, benchmarking RSP Engines is possible thanks to the proposed RDF streams, continuous queries, and performance measurements. The critics on these works have showed their limits and further developments partially went beyond \cite{paper paper}. However we noted that the community still lacks the formalization of a comparative approach for the research on RSP Engines and not least an infrastructure that allows the its rigorous application.

In this thesis we propose \namens, a framework that tries to solve the Stream Reasoning need of a Systematic Comparative Approach on RSP Engine evaluation.

\section{Why Comparative?}

The comparative research approach is typical of the Social Science, where the complexity of the subject go beyond the possible observable models. RSPEngine are complex system too, as can be seenin Chapter 6, is difficult to analyse them even in case of simple, well defined architectures. This emphasises the importance to be able to conduct comparative research based on controlled experimental conditions and, thus, the need for a open source16 framework like Heaven.

\section{Requirements} \label{sec:requirements}

We developed \name in order to simplify and support Systematic Comparative Research Approach on RSP engines. To this extent we need to answer the following questions: 
\begin{enumerate}
\item[Q.1] How can the behaviour of system be evaluated? 
\item[Q.2] What makes this evaluation rigorous? 
\item[Q.3] How can this rigorous evaluation be automated?
\end{enumerate}

A proper answer to Question Q.1 can be stated exploiting the traditional definition of \textit{experiment}: a test under controlled conditions that is made to demonstrate a known truth or examine the validity of an hypothesis. Going deeply, we answer Q.2 bringing about the notions of \textit{reproducibility}, \textit{repeatability}, and \textit{comparability} of experiments. The concepts we identified make easy to answer Q.3 formalising the technical requirements for \namens.

Reproducibility is related to the variation in measurements made on a subject under changing conditions. The concept of experiment gather this conditions. \name must allow its users to define it in details: 
\begin{enumerate}
\item[R.1] it must be \textit{test data independent}, thus allowing users to chose relevant RDF data streams and ontologies from their domain of interest. %R.2.1
\item[R.2] it must be \textit{query independent}, thus allowing users to register relevant queries from their domains of interest. %R.2.2
\item[R.3] it must be \textit{engine independent}, thus allowing users to put an RSP engine on the test stand by the means of easy to implement software interfaces, e.g., it should adopt an event-base architecture as normally done by RSP engines and present events to RSP engine in a simple to parse RDF serialisation. %R4 e R5
\item[R.4] it must include a \textit{basic set of performance measurements} \cite{DBLP:conf/esws/ScharrenbachUMVB13} including \textbf{Latency} -- defined as the delay between the injection of an event in the RSP engine and its response to it --, \textbf{Memory Usage} -- defined as the difference between total system memory and free memory --, and \textbf{Completeness \& Soundness} of query-answering results.  %R2.3
\item[R.5] it should enable users to extend the test stand adding their own software sensors in order to other performance measurements %???
\end{enumerate}

In terms of software engineering the list of requirements above demands an \textit{Extendable Design} [R6], i.e.,  the possibility to replace theoretically each module with one with the same interfaces, but different behaviour, without affecting architecture stability.

Repeatability of measurements regards the variation in repeat measurements made on the same subject under identical conditions. \name must not affect the RSP engine evaluation to grant it. This from a practical point of view poses two requirements to the test stand:
\begin{enumerate}
\item[R.7] it must not be running when the RSP engine is under execution. %R.1.2
\item[R.8] it must have reduced (and possibly constant) memory footprint. %R.1.1
\end{enumerate}

The comparative research is case-oriented. It allows the systematic analysis of complex cases, exploiting comparable metrics. The complex cases are seen as configurations, a combination of known properties, upon which is possible to identify parallelism or state contrasts. A Systematic Comparative Research Approach (SCRA) requires both the definition of \textit{Metrics that allow comparison} and the standardization of \textit{Evaluation conditions}.  \name must support the collection of the performance measurements as custom metrics [R.9]; again the concept of experiment is required as a formalization for the execution setting. The next step consists in providing \textit{Tools for qualitative analysis}, which allow visualisation of the results [R.10].

Moreover \textit{Initial terms of comparison}, a.k.a. baselines, are needed to guide the SCRA. A baseline is an elementary solution for the research problem, which maintains experimental validity and consequently is relevant as a term of comparison. Baselines can be exploited as a simple case-studies to support the need of \textit{Examples of successful analysis and evaluations}, which can be seen as guidelines

\name contains specific modules to fulfil this request, which of course presents their own requirements.

\begin{enumerate}
\item[B.R.1] They must be a solution: entirely solve the problem, maybe at the expense of the performances % devono risolvere interamente il problema, nel nostro caso devono essere complete e sound rispetto ad entailment regime che si è scelto
\item[B.R.3] They must be elementary: not represent a innovative solution w.r.t. the state of the art % non devono essere soluzioni smart, devono chiedere il minimo effort
\item[B.R.2] They must be valid: the performance measurements must be comparable %e' inutile se sono sopra un'ordine di grandezza ad esempio in latenza, rispetto ad una soluzione commerciale. probabilmente è sbagliata la domanda di ricerca
\item[B.R.2] They must be relevant: They must cover one of the most relevant variants in theoretical solution %la soluzione che coprono deve essere una delle immediate soluzioni del problema iniziale: Graph vs Statement o Inc vs Naive, tempo controllato esternamente, ecc
\item[B.R.4] They must %non devono offrire api più complesse rispetto a un RSPEngine tradizionale.
\end{enumerate}


